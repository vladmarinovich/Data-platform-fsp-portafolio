# Salvando Patitas Data Platform

Un pipeline ELT serverless dise√±ado para ingestar, transformar y analizar datos operativos para la fundaci√≥n *Salvando Patitas*. El sistema integra fuentes de Supabase (PostgreSQL) en un ecosistema Google Cloud Platform, utilizando **Cloud Run Jobs** para la extracci√≥n orquestada y **BigQuery + Dataform** para el almacenamiento y transformaciones de datos.

## Definici√≥n del Problema

La fundaci√≥n enfrentaba desaf√≠os con la fragmentaci√≥n de datos operativos a trav√©s de su aplicaci√≥n CRM personalizada. Acceder a insights hist√≥ricos requer√≠a extracciones manuales de datos, lo que llevaba a inconsistencias y reportes desactualizados.

Esta plataforma aborda estos problemas mediante:
*   **Centralizaci√≥n de Datos**: Unificaci√≥n de registros de donaciones, gastos y gesti√≥n de casos en una √∫nica fuente anal√≠tica de verdad.
*   **Consistencia**: Implementaci√≥n de estrategias robustas de carga incremental para capturar todos los cambios de datos sin procesamiento redundante.
*   **Confiabilidad Operativa**: Automatizaci√≥n del pipeline para ejecutarse diariamente con m√≠nima sobrecarga de mantenimiento, asegurando trazabilidad y manejo de errores.

## Arquitectura

La arquitectura sigue un patr√≥n ELT modular, aprovechando componentes serverless para minimizar costos operativos mientras se maximiza la escalabilidad.


![Diagrama de Arquitectura](docs/img/Flujo%20Pipeline.jpeg)

```
[Supabase (PostgreSQL)] 
       |
       | (Python ETL Container / Cloud Run Jobs)
       v
[Google Cloud Storage] <--- (Gesti√≥n de Estado / watermarks.json)
(Zona: Raw / Parquet)
       |
       | (Tablas Externas)
       v
[BigQuery: Capa Raw]
       |
       | (Ejecuci√≥n Dataform)
       v
[BigQuery: Capa Silver] ---> [Dashboard en Looker Studio]
```

#### Modelo de Datos Operativo (CRM)
Para dar contexto sobre la complejidad de la fuente de datos, este es el modelo relacional que nuestro pipeline ingesta y transforma:

![Modelo de Datos Operativo](docs/img/oltp-layer-modelo-de-base-de-datos-transacional.jpeg)


### Componentes Principales
1.  **Extracci√≥n (Python)**: Una aplicaci√≥n Python contenerizada extrae datos desde Supabase.
    *   **Tablas Incrementales**: Recupera solo registros modificados desde la √∫ltima marca de agua de ejecuci√≥n (persistida en GCS).
    *   **Tablas Snapshot**: Realiza recargas completas para tablas de dimensi√≥n peque√±as para asegurar integridad referencial.
    *   **Ingesta**: Los datos se escriben en GCS en formato Parquet particionado para un rendimiento de consulta √≥ptimo.
    
    *Flujo Interno del Extractor (Inicializaci√≥n y Estado):*
    ![ETL Init](docs/img/etl-unner-11-inicializacion-y-gestion-de-estado.jpeg)

    *Orquestaci√≥n de Tablas:*
    ![ETL Orchestration](docs/img/etl-runner-12-orquestacion-de-tablas.jpeg)
2.  **Almacenamiento (GCS & BigQuery)**: Google Cloud Storage act√∫a como el Data Lake. BigQuery monta estos archivos como Tablas Externas (Capa Raw).
3.  **Transformaci√≥n (Dataform)**: Pipelines SQLX transforman datos Raw hacia la capa Silver, aplicando limpieza, tipeo y l√≥gica de negocio.
4.  **Orquestaci√≥n**: Cloud Scheduler dispara el Job de Cloud Run diariamente.

## Ejecuci√≥n en Producci√≥n

El pipeline est√° desplegado como un contenedor Docker en **Google Cloud Run Jobs**.

*   **Trigger**: Cloud Scheduler inicia el trabajo diariamente a las **07:00 AM (America/Santiago)**.
*   **Ejecuci√≥n del Job**:
    1.  El contenedor inicia y carga la configuraci√≥n desde variables de entorno.
    2.  Recupera el estado actual (`watermarks.json`) desde GCS.
    3.  Realiza la extracci√≥n incremental para tablas de alto volumen (`donaciones`, `gastos`, `casos`) y extracci√≥n snapshot para cat√°logos.
    4.  Tras la subida exitosa a GCS, actualiza el estado de la marca de agua.
    5.  (Integraci√≥n Conectada) Dataform ejecuta las transformaciones posteriores.
*   **Monitoreo**: Logs de ejecuci√≥n, m√©tricas de volumen de datos y trazas de errores son capturados en Cloud Logging.

## Visualizaci√≥n y Documentaci√≥n

*   **[Dashboard en Looker Studio](https://lookerstudio.google.com/u/0/reporting/cb2392ff-d151-4b16-9bc3-49df863ced2c/page/p_97ri4w4xyd)**
    *   Muestra la salida final del pipeline. Los evaluadores pueden verificar la frescura de los datos, agregaciones y la aplicaci√≥n pr√°ctica de las capas de datos Gold/Silver.

*   **[Diagrama de Arquitectura y Sistemas (Miro)](https://miro.com/welcomeonboard/UkduTDRzZFZlSW9xek1EL2dwRG1XVG8rQmRvcVFWbGhRMEhjVHBmUnU5MSs0ek5LdlZxSHcyOE15UXNydlNkOHQ1N3ROTEdEd2dQOVhEcDN4MlF6S0d0WEJySWE5c2xhNGNnVHB1WXRGNGl2OWJZNlhydU00bWVoOFRZK095bkNhWWluRVAxeXRuUUgwWDl3Mk1qRGVRPT0hdjE=?share_link_id=538214555000)**
    *   Representaci√≥n visual detallada de los componentes del sistema, flujo de datos e interacciones entre el CRM, el pipeline ETL y la capa de visualizaci√≥n.

## Decisiones T√©cnicas Clave

*   **Cloud Run Jobs para ETL**: Seleccionado por su naturaleza serverless. El tiempo de inicio es r√°pido y la facturaci√≥n es por segundo. A diferencia de Cloud Functions, maneja tiempos de espera de validaci√≥n m√°s largos y procesamiento por lotes intensivo en memoria con gracia. A diferencia de Dataproc, requiere cero gesti√≥n de cl√∫steres.
*   **BigQuery**: Elegido por su separaci√≥n de almacenamiento y c√≥mputo. Permite consultar archivos Parquet crudos directamente desde GCS sin costos de carga, y escala sin esfuerzo para consultas anal√≠ticas.
*   **Dataform**: Proporciona mejores pr√°cticas de ingenier√≠a de software a la transformaci√≥n SQL (CI/CD, control de versiones, gesti√≥n de dependencias y pruebas de aserci√≥n), superior a la gesti√≥n de scripts SQL crudos programados v√≠a cron.
*   **Carga Incremental con Estado Persistente**: Esencial para escalar. En lugar de recargar todo el conjunto de datos diariamente, el sistema rastrea la marca de tiempo `last_modified_at`. Esto reduce los costos de egreso de Supabase y el tiempo de procesamiento de minutos a segundos para deltas diarios.

## Estado de la Plataforma

*   **Implementado**:
    *   ‚úÖ Pipeline de extracci√≥n completo (Python/Docker).
    *   ‚úÖ Capa de almacenamiento (GCS Parquet + BigQuery Raw).
    *   ‚úÖ L√≥gica de transformaci√≥n (Capa Silver Dataform).
    *   ‚úÖ Orquestaci√≥n (Cloud Run + Scheduler).
    *   ‚úÖ Visualizaci√≥n (Dashboard B√°sico).

*   **Pendiente**:
    *   üöß Modelado de Capa Gold (Esquema Estrella).
    *   üöß Aserciones de Calidad de Datos Avanzadas (Nivel Gold).
    *   üöß Integraci√≥n ML.

## Pr√≥ximos Pasos (Roadmap)

*   **Implementaci√≥n Capa Gold**: Desarrollar modelos dimensionales finales optimizados para herramientas de BI.
*   **Aserciones Estrictas**: Implementar pruebas de conteo de filas y distribuci√≥n para bloquear datos incorrectos antes de que lleguen a la capa Gold.
*   **Integraci√≥n Vertex AI**: Desplegar modelos de ML para predecir tendencias de donaci√≥n basadas en datos hist√≥ricos.
*   **Interfaz Ag√©ntica**: Implementar un agente basado en LLM para permitir consultas en lenguaje natural del conjunto de datos.
*   **Exposici√≥n API**: Crear una capa API ligera para servir m√©tricas procesadas de vuelta al CRM operativo.
