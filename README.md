# ğŸ¾ Salvando Patitas Data Platform (SPDP)

> **IngenierÃ­a de Datos aplicada al Bienestar Animal.**  
> *Una plataforma moderna, escalable y automatizada para optimizar la gestiÃ³n de rescates, donaciones y recursos.*

---

## ğŸ“– Sobre el Proyecto: Full-Stack Data Engineering

**Salvando Patitas** es una fundaciÃ³n dedicada al rescate y rehabilitaciÃ³n de animales. Este proyecto no es solo una plataforma de datos; es una **soluciÃ³n integral (End-to-End)** diseÃ±ada, construida y operada por un solo ingeniero con visiÃ³n de negocio.

### ğŸŒŸ Diferenciadores Clave
1.  **VisiÃ³n Full-Stack & Autoria Total:** 
    *   No solo ingestamos datos; **construimos la fuente**. El sistema operativo de la fundaciÃ³n (CRM) fue desarrollado a medida utilizando **Django y React**.
    *   Esto garantiza un conocimiento profundo del dato desde su creaciÃ³n por el usuario final hasta su explotaciÃ³n en los dashboards de BI, eliminando las cajas negras.
2.  **Eficiencia de Costos Radical (Cloud FinOps):**
    *   Arquitectura optimizada para operar con **menos de $0.01 USD diarios**.
    *   Uso estratÃ©gico de Cloud Run Jobs (Serverless) y BigQuery On-Demand para maximizar el ROI de una organizaciÃ³n sin fines de lucro.
    *   *Pensamiento de Ingeniero Industrial aplicado a la Nube: MÃ¡ximo valor, mÃ­nimo desperdicio.*

Este repositorio contiene la implementaciÃ³n del **Data Lakehouse** que alimenta la inteligencia de negocio de la fundaciÃ³n, respondiendo preguntas crÃ­ticas sobre eficiencia operativa y sostenibilidad financiera.

---

## ğŸ› ï¸ Tech Stack & Habilidades

Este proyecto demuestra competencias en el **Modern Data Stack**:

*   **Lenguajes:** ğŸ Python (ETL), ğŸ“œ SQLX (Dataform), ğŸš Bash.
*   **Cloud (GCP):** â˜ï¸ Google Cloud Storage (Data Lake), ğŸ” BigQuery (Data Warehouse).
*   **OrquestaciÃ³n & TransformaciÃ³n:** ğŸ—ï¸ Google Dataform (CI/CD para SQL), Modular Python Scripts.
*   **Fuentes de Datos:** âš¡ Supabase (PostgreSQL).
*   **Buenas PrÃ¡cticas:**
    *   Arquitectura **Medallion** (Bronze ğŸ¥‰ -> Silver ğŸ¥ˆ -> Gold ğŸ¥‡).
    *   **Data Quality Testing** (Assertions automÃ¡ticos).
    *   Estrategias de carga **Incremental** y **Snapshot**.
    *   Infraestructura como CÃ³digo (IaC).

---

## ğŸ—ï¸ Arquitectura de la SoluciÃ³n

El flujo de datos estÃ¡ diseÃ±ado para ser robusto, idempotente y fÃ¡cil de mantener:

```mermaid
graph LR
    A[Supabase API] -->|Python Extract| B(GCS Bronze Layer)
    B -->|Dataform Load| C(BigQuery Raw)
    C -->|Dataform Transform| D(BigQuery Silver)
    D -->|Dataform Model| E(BigQuery Gold)
    E -->|Analytics| F[Looker/Tableau]
```

### Componentes Clave

| Componente | DescripciÃ³n | Herramienta |
| :--- | :--- | :--- |
| **Ingesta (ETL)** | Scripts modulares en Python con paginaciÃ³n, manejo de tipos estrictos y control de watermarks. | `src/etl/` |
| **Data Lake** | Almacenamiento costo-eficiente en parquets particionados y snapshots. | GCS |
| **TransformaciÃ³n** | Pipeline ELT con gestiÃ³n de dependencias, grafos de ejecuciÃ³n y testing. | Dataform |
| **Calidad** | Reglas de negocio (Assertions) para validar unicidad, integridad referencial y nulos. | SQLX |

---

## ğŸ“‚ Estructura del Repositorio

```text
/
â”œâ”€â”€ src/                 # ğŸ LÃ³gica de ExtracciÃ³n (Python Modular)
â”‚   â”œâ”€â”€ etl/             # Config, Connect, Extract, Transform, Load
â”‚   â””â”€â”€ main.py          # Orquestador del pipeline
â”œâ”€â”€ dataform/            # ğŸ—ï¸ LÃ³gica de TransformaciÃ³n (SQLX)
â”‚   â”œâ”€â”€ definitions/     # Modelos (Raw, Silver, Gold, Assertions)
â”‚   â””â”€â”€ workflow.yaml    # ConfiguraciÃ³n del pipeline
â”œâ”€â”€ scripts/             # ğŸ› ï¸ Herramientas de Mantenimiento (Backfill, Cleaning)
â”œâ”€â”€ docs/                # ğŸ“š DocumentaciÃ³n TÃ©cnica Detallada
â””â”€â”€ .github/             # ğŸ¤– CI/CD Pipelines (IntegraciÃ³n con GCP)
```

## ğŸ“š DocumentaciÃ³n Detallada

Si deseas profundizar en los aspectos tÃ©cnicos, consulta:

*   **[ğŸ—ï¸ Arquitectura y Estrategias de Carga](docs/ARCHITECTURE.md)** (Incremental vs Full Load).
*   **[ğŸ› ï¸ Manual de Operaciones y Mantenimiento](docs/MAINTENANCE.md)**.
*   **[ğŸš‘ Log de ResoluciÃ³n de Problemas](docs/TROUBLESHOOTING.md)**.

---

## ğŸš€ CÃ³mo Ejecutar (Local)

1.  **Clonar y configurar entorno:**
    ```bash
    git clone https://github.com/vladmarinovich/Data-platform-fsp-portafolio.git
    cd Data-platform-fsp-portafolio
    python3 -m venv .venv
    source .venv/bin/activate
    pip install -r requirements.txt
    ```

2.  **Correr Pipeline de Ingesta:**
    ```bash
    python3 -m src.main
    ```


---

## ğŸš€ AutomatizaciÃ³n y Despliegue (ETL Pipeline)

El proceso de ingesta ha sido contenerizado y automatizado para ejecutarse diariamente de forma desasistida ("Serverless").

### ğŸ—ï¸ Arquitectura de Despliegue
1.  **Docker:** Empaquetamos la lÃ³gica Python (`src/`) y sus dependencias en una imagen ligera (`python:3.12-slim`).
2.  **Artifact Registry:** Repositorio privado en GCP donde se almacenan las versiones de la imagen.
3.  **Cloud Run Jobs:** Ejecuta el contenedor bajo demanda. Ideal para tareas batch que tienen un inicio y un fin.
4.  **Cloud Scheduler:** "El Despertador". Activa el Job de Cloud Run todos los dÃ­as a las **07:00 AM**.

### ğŸ”„ CI/CD Manual (Deployment)
Para actualizar el cÃ³digo de producciÃ³n, utilizamos **Google Cloud Build** para construir la imagen en la nube sin dependencias locales:

```bash
# 1. Construir y Subir Imagen (Build & Push)
gcloud builds submit --tag us-central1-docker.pkg.dev/fsp-pipeline-project/spdp-repo/etl-runner:latest .

# 2. El Cloud Run Job detectarÃ¡ automÃ¡ticamente la etiqueta 'latest' en la prÃ³xima ejecuciÃ³n.
# (Opcional) Para forzar una ejecuciÃ³n manual inmediata:
gcloud run jobs execute etl-runner-job --region southamerica-west1
```

---

DiseÃ±ado y desarrollado por **Vladislav Marinovich**.  
*Transformando datos en segundas oportunidades.* ï¿½
