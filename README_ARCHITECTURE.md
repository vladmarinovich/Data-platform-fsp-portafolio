# üèóÔ∏è Arquitectura de Datos: SPDP (Salvando Patitas Data Platform)

Este documento describe la estrategia de extracci√≥n y carga (ELT) dise√±ada para garantizar consistencia, escalabilidad y simplicidad en el Data Lake.

---

## üîÑ Estrategias de Extracci√≥n

Utilizamos una arquitectura h√≠brida dependiendo de la naturaleza de los datos:

### 1. Tablas Transaccionales (Estrategia Incremental)
*   **Tablas:** `casos`, `donaciones`, `gastos`, `donantes`.
*   **Comportamiento:**
    *   Los datos crecen constantemente.
    *   Usamos una columna "Watermark" (`last_modified_at`) para bajar solo lo nuevo o modificado.
    *   **Almacenamiento:** Particionado por fecha de ingesti√≥n (`y=YYYY/m=MM/d=DD`).
    *   **Objetivo:** Mantener un historial completo e inmutable de todos los cambios.

### 2. Tablas Maestras/Cat√°logos (Estrategia Snapshot)
*   **Tablas:** `proveedores`, `hogar_de_paso`.
*   **Comportamiento:**
    *   Datos est√°ticos o de cambio lento (pocos registros, actualizaciones espor√°dicas).
    *   No se requiere historial de cambios d√≠a a d√≠a en la capa Raw.
    *   **Almacenamiento:** Ruta est√°tica `.../latest/tabla.parquet`.
    *   **L√≥gica:** **Sobreescritura (Overwrite)**. Cada ejecuci√≥n reemplaza el archivo anterior.
    *   **Objetivo:** Evitar duplicados en BigQuery. La tabla externa siempre apunta al archivo √∫nico "latest".

---

## üõ†Ô∏è Gu√≠a de Tipos de Datos (The "Iron Rules")

Para evitar errores de "Type Mismatch" en BigQuery, el extractor aplica conversiones estrictas:

| Concepto | Tipo Parquet/BQ | Regla Python |
| :--- | :--- | :--- |
| **IDs** | `INT64` (Nullable) | `to_numeric(..., errors='coerce').astype('Int64')` |
| **Montos** | `FLOAT64` | `to_numeric(..., errors='coerce').astype('float64')` |
| **Fechas** | `TIMESTAMP` | `to_datetime(..., errors='coerce')` |
| **Texto** | `STRING` | `astype(str)` + limpieza de `"nan"` a `NULL` |
| **JSON** | `STRING` | Convertido a texto para evitar estructuras anidadas complejas. |

---

## üöÄ Flujo de Trabajo (Pipeline)

1.  **Extract:** Python descarga datos de la API de Supabase (usando `postgrest`).
2.  **Transform:** Pandas limpia tipos y convierte nulos.
3.  **Load:**
    *   Si es Incremental -> Sube a partici√≥n diaria nueva.
    *   Si es Snapshot -> Sube a carpeta `latest/` (sobrescribiendo).
4.  **BigQuery:** Tablas Externas leen directamente desde GCS.
    *   *Nota:* Las tablas Snapshot no deben tener particionamiento Hive en su definici√≥n DDL.

---

## üö® Troubleshooting Com√∫n

*   **Duplicados en Proveedores:** Revisa si la tabla externa est√° leyendo particiones viejas (`y=2025...`) junto con el snapshot (`latest/`). Soluci√≥n: Borrar particiones viejas en GCS.
*   **Error "Type Double vs Int64":** Significa que hay archivos viejos con esquema sucio. Soluci√≥n: Borrar bucket de la tabla y recargar.

---
**Owner:** Ingenier√≠a de Datos SPDP  
**√öltima Actualizaci√≥n:** Diciembre 2025
